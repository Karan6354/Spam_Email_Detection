{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # For data manipulation\n",
    "import pickle  # For loading the saved model and vectorizer\n",
    "import nltk  # For natural language processing\n",
    "from nltk.corpus import stopwords  # For stopword removal\n",
    "from nltk.tokenize import word_tokenize  # For tokenizing the text\n",
    "from nltk.stem.porter import PorterStemmer  # For stemming the words\n",
    "import string  # For handling punctuation\n",
    "\n",
    "# Initialize PorterStemmer for stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')  # Stopwords dataset\n",
    "nltk.download('punkt')  # Punkt tokenizer for tokenization\n",
    "\n",
    "# Function to preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by tokenizing, converting to lowercase,\n",
    "    removing non-alphanumeric tokens, stopwords, and punctuation.\n",
    "    Optionally, stemming can be applied (commented out here).\n",
    "    \"\"\"\n",
    "    # Tokenize the text, convert to lowercase, and remove non-alphanumeric tokens\n",
    "    tokens = [word for word in nltk.word_tokenize(text.lower()) if word.isalnum()]\n",
    "    \n",
    "    # Remove stopwords (common words like 'the', 'and', etc.) and punctuation\n",
    "    stop_words = set(stopwords.words('english'))  # Set of stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "    # Optional: Apply stemming to each token (currently commented out)\n",
    "    # tokens = [ps.stem(word) for word in tokens]\n",
    "    \n",
    "    # Return the processed text as a string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Load the saved machine learning model and vectorizer\n",
    "# Ensure to update the path to where your files are stored\n",
    "with open('path_to_model/spam_svm_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)  # Load the trained SVM model\n",
    "\n",
    "with open('path_to_model/tfidf_vectorizer.pkl', 'rb') as vec_file:\n",
    "    loaded_vectorizer = pickle.load(vec_file)  # Load the trained TF-IDF vectorizer\n",
    "\n",
    "# Upload the CSV file and load it into a DataFrame\n",
    "# You can replace 'your_file.csv' with the actual path to your CSV file\n",
    "csv_file_path = 'your_file.csv'  # Update this path with your actual CSV file\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# If the CSV doesn't have a 'label' column, we can handle that too just uncomment below 2 lines:\n",
    "# if 'label' not in data.columns:\n",
    "#     data['label'] = None  # Adding a 'label' column with None if it doesn't exist\n",
    "\n",
    "# Initialize an empty list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterate through the DataFrame to preprocess and predict each email's label\n",
    "for index, row in data.iterrows():\n",
    "    email_text = row['text']  # Get the text of the email\n",
    "    \n",
    "    # Preprocess the email text (tokenize, remove stopwords, etc.)\n",
    "    preprocessed_text = preprocess_text(email_text)\n",
    "    \n",
    "    # Vectorize the preprocessed text using the loaded TF-IDF vectorizer\n",
    "    email_vectorized = loaded_vectorizer.transform([preprocessed_text]).toarray()\n",
    "    \n",
    "    # Predict the label (spam or ham) using the loaded SVM model\n",
    "    prediction = loaded_model.predict(email_vectorized)\n",
    "    \n",
    "    # Add the prediction (either 1 or 0) to the predictions list\n",
    "    predictions.append(prediction[0])\n",
    "\n",
    "# Add the predictions as a new column 'predicted_label' to the DataFrame\n",
    "data['predicted_label'] = predictions\n",
    "\n",
    "# Show the results, displaying the original text, actual label (if available), and predicted label\n",
    "print(data[['text', 'label', 'predicted_label']])\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
